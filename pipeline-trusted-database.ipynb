{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4516dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-01 18:16:48--  https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar\r\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 146.75.36.209, 2a04:4e42:78::209\r\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|146.75.36.209|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2862 (2.8K) [application/java-archive]\r\n",
      "Saving to: ‘aws-java-sdk-1.11.30.jar.1’\r\n",
      "\r\n",
      "100%[======================================>] 2,862       --.-K/s   in 0s      \r\n",
      "\r\n",
      "2024-05-01 18:16:48 (54.1 MB/s) - ‘aws-java-sdk-1.11.30.jar.1’ saved [2862/2862]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!sudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab25654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-01 18:16:50--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 146.75.28.209, 2a04:4e42:78::209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|146.75.28.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 126287 (123K) [application/java-archive]\n",
      "Saving to: ‘hadoop-aws-2.7.3.jar.1’\n",
      "\n",
      "100%[======================================>] 126,287     --.-K/s   in 0.005s  \n",
      "\n",
      "2024-05-01 18:16:50 (24.9 MB/s) - ‘hadoop-aws-2.7.3.jar.1’ saved [126287/126287]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9f021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-01 18:16:51--  https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar\r\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 146.75.28.209, 2a04:4e42:78::209\r\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|146.75.28.209|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2046361 (2.0M) [application/java-archive]\r\n",
      "Saving to: ‘jets3t-0.9.4.jar.1’\r\n",
      "\r\n",
      "100%[======================================>] 2,046,361   --.-K/s   in 0.01s   \r\n",
      "\r\n",
      "2024-05-01 18:16:51 (176 MB/s) - ‘jets3t-0.9.4.jar.1’ saved [2046361/2046361]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!sudo wget https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b226dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip install with root privileges is generally not a good idea. Try `python3 -m pip install --user` instead.\u001b[0m\n",
      "Requirement already satisfied: pymongo in /usr/local/lib64/python3.7/site-packages (4.7.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from pymongo) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7409ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip install with root privileges is generally not a good idea. Try `python3 -m pip install --user` instead.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement hadoop-aws (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for hadoop-aws\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hadoop-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1d1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SparkContext\n",
    "import pymongo\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark as pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403a7ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_3297/2799132669.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3297/3972298485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.jars.packages'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'org.apache.hadoop:hadoop-aws:3.2.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mcallsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             \u001b[0mcallsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                             \u001b[0mcallsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinenum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m                         )\n\u001b[1;32m    456\u001b[0m                     )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_3297/2799132669.py:2 "
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cae67ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd432b0b1d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fe1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição do esquema do JSON\n",
    "json_schema = StructType([\n",
    "    StructField(\"dispositivoId\", StringType(), True),\n",
    "    StructField(\"dispositivo\", StringType(), True),\n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"unidadeMedida\", StringType(), True),\n",
    "    StructField(\"conteudoAdicional\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442d402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do S3\n",
    "s3_bucket = \"s3a://raw-souths-eagle-ivan\"\n",
    "s3_access_key = \"ASIA2L45MTMD3PKC6AIJ\"\n",
    "s3_secret_key = \"hNZMTJjIfZ8B+ai1H4RhOw6ziepOlDPmeh04uHhf\"\n",
    "aws_session_token = \"IQoJb3JpZ2luX2VjEPH//////////wEaCXVzLXdlc3QtMiJIMEYCIQDTAFtA60+0okic5jYGztQusilsYuctjBAgNvJsFWsCVwIhAKleksfqNqJirFJWTxNUsGQX3TUUGyqzMnTjJkeeBfzlKrQCCEoQARoMNzEyNzU3Nzc5MjA3IgydCE+KrIlr9O7U4SYqkQLWkRRHT/647rYN4ZcD7aoXgmg999IsOFf1ejzeO59M2DPZ/QpkDAA3+qOJ1WzU4XhpnvXyjeHQ93Kh3RyJGibFAuIH5NaNtC0DDdvaCc+DQLP+HnLRVEwC2iAKw/62Y4RJryPEtC3O4kwa3fq7zrH+YqfJbsYhr8ZQyQujSCW3ie9s4l93TnJwxLgrtBZnT2L5XBtsOU8Dau95bfNvBlOl43XNhXQnAyWoCUMS6l1iALFbNYfC8RPKtSMCClf5jBuPK312cOsjqOv1C5TIIErnyfEMgx3/4b41wgnkwcU7dWbHlqMdkfS+VzRywnrovHOiBr31QL2MAlSfnDT2Y26sq33cTyVQAzxR0oag2mISZbownu7JsQY6nAE2RBFcDqhNQr9/BGPiD2UL88xfaRgSe+eQ7YM4gEVHurW9BCvMQVbAp2vThCEl2jypIhBtwTu/24m4bIJHEpwW8le7W+uuP4e0khNFD5mQ5nwkNI33CAIcI1UB1AjSZXqS0mxp46PYFPQZhHjjZoO11K2kIs8PN/x8gOuvCBMhpw0JTs/bZ/0gQzt+KeHZOtA3UlP6rrUKEV9VDbQ=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9e3a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', s3_access_key)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', s3_secret_key)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.session.token', aws_session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a33e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações de conexão do MongoDB\n",
    "mongo_uri = \"mongodb+srv://tx-adm-cl:7BoVvnfOXTCmZ5l8@tx-cluster.2p3coke.mongodb.net/?retryWrites=true&w=majority&appName=tx-cluster\"\n",
    "mongo_database = \"spring-iot\"\n",
    "mongo_collection = \"eletroencefalograma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3337c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para inserir os dados no MongoDB\n",
    "def write_to_mongodb(rows):\n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "    db = client[mongo_database]\n",
    "    collection = db[mongo_collection]\n",
    "    for row in rows:\n",
    "        data = {\"json_data\": row}\n",
    "        collection.insert_one(data)\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina a ação para processar os dados\n",
    "query = source_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(lambda batch_df, batch_id: write_to_mongodb(batch_df.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicie a consulta\n",
    "query.start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
